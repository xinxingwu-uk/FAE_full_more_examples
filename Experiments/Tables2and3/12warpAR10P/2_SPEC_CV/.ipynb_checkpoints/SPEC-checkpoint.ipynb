{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import random\n",
    "import scipy.sparse as sparse\n",
    "import scipy.io\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skfeature.utility import construct_W\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skfeature.function.similarity_based import SPEC\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer \n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sparse\n",
    "from keras.datasets import fashion_mnist\n",
    "import scipy.io\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"../Defined\")\n",
    "import Functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../Dataset/warpAR10P.mat\"\n",
    "Data = scipy.io.loadmat(data_path)\n",
    "\n",
    "data_arr_=Data['X'].astype('float32')/255\n",
    "label_arr=Data['Y'][:, 0]\n",
    "\n",
    "Data=MinMaxScaler(feature_range=(0,1)).fit_transform(data_arr_)\n",
    "\n",
    "data_arr=Data\n",
    "label_arr_onehot=label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 .Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR\n",
    " \n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def cal(p_data_arr,\\\n",
    "        p_label_arr_onehot,\\\n",
    "        p_key_feture_number,\\\n",
    "        p_seed):\n",
    "    \n",
    "    C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(p_data_arr,p_label_arr_onehot,test_size=0.2,random_state=p_seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(p_seed)\n",
    "    np.random.seed(p_seed)\n",
    "    rn.seed(p_seed)\n",
    "    \n",
    "    train_feature=C_train_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_x\n",
    "    test_label=C_test_y\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Construct affinity matrix\n",
    "    kwargs = {'style': 0}\n",
    "    \n",
    "    t_start = time.time()\n",
    "\n",
    "    # Obtain the scores of features, and sort the feature scores in an ascending order according to the feature scores\n",
    "    train_score = SPEC.spec(train_feature, **kwargs)\n",
    "\n",
    "    train_idx = SPEC.feature_ranking(train_score, **kwargs)\n",
    "    \n",
    "    t_used=time.time() - t_start\n",
    "    \n",
    "    write_to_csv(np.array([t_used]),\"./log/SPEC_time.csv\")\n",
    "\n",
    "    C_train_selected_x = train_feature[:, train_idx[0:key_feture_number]]\n",
    "\n",
    "    # Obtain the scores of features, and sort the feature scores in an ascending order according to the feature scores\n",
    "    test_score = SPEC.spec(test_feature, **kwargs)\n",
    "\n",
    "    test_idx = SPEC.feature_ranking(test_score, **kwargs)\n",
    "\n",
    "    # Obtain the dataset on the selected features\n",
    "    C_test_selected_x = test_feature[:, test_idx[0:key_feture_number]]\n",
    "\n",
    "    # Classification on original features\n",
    "    train_feature=C_train_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_x\n",
    "    test_label=C_test_y\n",
    "    \n",
    "    orig_train_acc,orig_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "    \n",
    "    # Classification on selected features\n",
    "    \n",
    "    train_feature=C_train_selected_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_selected_x\n",
    "    test_label=C_test_y\n",
    "\n",
    "    selec_train_acc,selec_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "\n",
    "    # Linear reconstruction\n",
    "    train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "    test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "    reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "    results=np.array([orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss])\n",
    "    \n",
    "    write_to_csv(results.reshape(1,len(results)),\"./log/SPEC_results.csv\")\n",
    "    \n",
    "    return orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_arr=data_arr\n",
    "p_label_arr_onehot=label_arr_onehot\n",
    "p_key_feture_number=key_feture_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1597222222222222\n",
      "Testing accuracy： 0.1597222222222222\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1909722222222222\n",
      "Testing accuracy： 0.1909722222222222\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.125\n",
      "Testing accuracy： 0.125\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.19444444444444445\n",
      "Testing accuracy： 0.19444444444444445\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.24305555555555555\n",
      "Testing accuracy： 0.24305555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.2465277777777778\n",
      "Testing accuracy： 0.2465277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1875\n",
      "Testing accuracy： 0.1875\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.09375\n",
      "Testing accuracy： 0.09375\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9895833333333334\n",
      "Testing accuracy： 0.9895833333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.15625\n",
      "Testing accuracy： 0.15625\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.19791666666666666\n",
      "Testing accuracy： 0.19791666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.16666666666666666\n",
      "Testing accuracy： 0.16666666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.09722222222222222\n",
      "Testing accuracy： 0.09722222222222222\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.04861111111111111\n",
      "Testing accuracy： 0.04861111111111111\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.07291666666666667\n",
      "Testing accuracy： 0.07291666666666667\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1111111111111111\n",
      "Testing accuracy： 0.1111111111111111\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.15625\n",
      "Testing accuracy： 0.15625\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.125\n",
      "Testing accuracy： 0.125\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.16319444444444445\n",
      "Testing accuracy： 0.16319444444444445\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Testing accuracy： 0.13541666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1423611111111111\n",
      "Testing accuracy： 0.1423611111111111\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.2048611111111111\n",
      "Testing accuracy： 0.2048611111111111\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.10069444444444445\n",
      "Testing accuracy： 0.10069444444444445\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.17708333333333334\n",
      "Testing accuracy： 0.17708333333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9930555555555556\n",
      "Testing accuracy： 0.9930555555555556\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1527777777777778\n",
      "Testing accuracy： 0.1527777777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1597222222222222\n",
      "Testing accuracy： 0.1597222222222222\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Testing accuracy： 0.1701388888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1284722222222222\n",
      "Testing accuracy： 0.1284722222222222\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Testing accuracy： 0.14930555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Testing accuracy： 0.9965277777777778\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.19791666666666666\n",
      "Testing accuracy： 0.19791666666666666\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Testing accuracy： 0.1388888888888889\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.125\n",
      "Testing accuracy： 0.125\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Testing accuracy： 0.14583333333333334\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.08680555555555555\n",
      "Testing accuracy： 0.08680555555555555\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Testing accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.20833333333333334\n",
      "Testing accuracy： 0.20833333333333334\n"
     ]
    }
   ],
   "source": [
    "for p_seed in np.arange(0,50):\n",
    "    orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss=cal(p_data_arr,\\\n",
    "                                                                                        p_label_arr_onehot,\\\n",
    "                                                                                        p_key_feture_number,\\\n",
    "                                                                                        p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
