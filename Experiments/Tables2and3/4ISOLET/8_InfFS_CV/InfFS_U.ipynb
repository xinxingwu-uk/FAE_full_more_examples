{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import scipy.io\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"../Defined\")\n",
    "import Functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame=np.array(pd.read_csv('../Dataset/isolet1+2+3+4.data',header=None))\n",
    "test_data_frame=np.array(pd.read_csv('../Dataset/isolet5.data',header=None))\n",
    "\n",
    "train_data_arr=(train_data_frame[:,0:617]).copy()\n",
    "train_label_arr=((train_data_frame[:,617]).copy()-1)\n",
    "test_data_arr=(test_data_frame[:,0:617]).copy()\n",
    "test_label_arr=((test_data_frame[:,617]).copy()-1)\n",
    "\n",
    "data_arr=MinMaxScaler(feature_range=(0,1)).fit_transform(np.r_[train_data_arr,test_data_arr])\n",
    "\n",
    "label_arr_onehot=np.r_[train_label_arr,test_label_arr]#to_categorical(train_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def IsnanAndIsinf(p_data):\n",
    "    p_data=np.array(p_data)\n",
    "    for i in np.arange(p_data.shape[0]):\n",
    "        for j in np.arange(p_data.shape[1]):\n",
    "            if np.isnan(p_data[i,j]) or np.isinf(p_data[i,j]):\n",
    "                p_data[i,j]=0\n",
    "    return p_data\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def InfFS(p_data_arr,p_alpha,use_specify_number=False,specify_number=50):\n",
    "    df = pd.DataFrame(p_data_arr)\n",
    "    corr_ij_spearman__=df.corr(method ='spearman')\n",
    "    corr_ij_spearman_=IsnanAndIsinf(corr_ij_spearman__)\n",
    "    corr_ij_spearman=1-np.abs(corr_ij_spearman_)\n",
    "    \n",
    "    STD=np.std(p_data_arr,axis=0)\n",
    "    \n",
    "    STDMatrix_=np.zeros((STD.shape[0],STD.shape[0]))\n",
    "    for i in np.arange(STD.shape[0]):\n",
    "        for j in np.arange(STD.shape[0]):\n",
    "            STDMatrix_[i,j]=max(STD[i],STD[j])\n",
    "            \n",
    "    STDMatrix_min=STDMatrix_-np.min(STDMatrix_)\n",
    "    STDMatrix_max=np.max(STDMatrix_min)\n",
    "    STDMatrix__=STDMatrix_min/STDMatrix_max\n",
    "    \n",
    "    STDMatrix=IsnanAndIsinf(STDMatrix__)\n",
    "    \n",
    "    N=p_data_arr.shape[1]\n",
    "    \n",
    "    eps = (5e-06) * N;\n",
    "    factor = 1 - eps\n",
    "    \n",
    "    A =  ( p_alpha*STDMatrix + (1-p_alpha)*corr_ij_spearman )\n",
    "\n",
    "    rho = np.max(np.sum(A,axis=1))\n",
    "\n",
    "    A = A / (rho+eps)\n",
    "    \n",
    "    I = np.eye(A.shape[0])\n",
    "    \n",
    "    r = factor/rho\n",
    "    \n",
    "    y = I - ( r * A )\n",
    "    S=np.linalg.inv(y)\n",
    "    \n",
    "    WEIGHT = np.sum( S , axis=1 )\n",
    "    RANKED=np.argsort(-WEIGHT)\n",
    "    \n",
    "    RANKED = RANKED\n",
    "    WEIGHT = WEIGHT\n",
    "    \n",
    "    e = np.ones(N)\n",
    "    t = np.dot(S, e)\n",
    "\n",
    "    nbins = 0.5*N\n",
    "    \n",
    "    cnts, bins = np.histogram(t, bins=int(nbins))\n",
    "    \n",
    "    thr =np.mean(cnts)\n",
    "    \n",
    "    size_sub = np.sum(cnts>thr)\n",
    "    \n",
    "    if use_specify_number:\n",
    "        size_sub=specify_number\n",
    "    \n",
    "    SUBSET = RANKED[0:size_sub]\n",
    "    \n",
    "    return SUBSET\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def cal(p_data_arr,\\\n",
    "        p_label_arr_onehot,\\\n",
    "        p_key_feture_number,\\\n",
    "        p_seed):\n",
    "    C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(p_data_arr,p_label_arr_onehot,test_size=0.2,random_state=p_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(p_seed)\n",
    "    np.random.seed(p_seed)\n",
    "    rn.seed(p_seed)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    train_feature=C_train_x\n",
    "    test_feature=C_test_x\n",
    "\n",
    "    t_start = time.time()\n",
    "    \n",
    "    train_idx=InfFS(train_feature,p_alpha,use_specify_number=True,specify_number=p_key_feture_number)\n",
    "\n",
    "    t_used=time.time() - t_start\n",
    "    \n",
    "    write_to_csv(np.array([t_used]),\"./log/InfFS_time.csv\")\n",
    "    \n",
    "    C_train_selected_x = train_feature[:, train_idx]\n",
    "    \n",
    "    test_idx=InfFS(test_feature,p_alpha,use_specify_number=True,specify_number=p_key_feture_number)\n",
    "    C_test_selected_x = test_feature[:, test_idx]\n",
    "\n",
    "    # Classification on original features\n",
    "    train_feature=C_train_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_x\n",
    "    test_label=C_test_y\n",
    "    \n",
    "    orig_train_acc,orig_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "    \n",
    "    # Classification on selected features\n",
    "    train_feature=C_train_selected_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_selected_x\n",
    "    test_label=C_test_y\n",
    "\n",
    "    selec_train_acc,selec_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "\n",
    "    # Linear reconstruction\n",
    "    train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "    test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "    reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "    results=np.array([orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss])\n",
    "    \n",
    "    write_to_csv(results.reshape(1,len(results)),\"./log/InfFS_results.csv\")\n",
    "    \n",
    "    return orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_arr=data_arr\n",
    "p_alpha=0.5\n",
    "p_label_arr_onehot=label_arr_onehot\n",
    "p_key_feture_number=key_feture_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9493589743589743\n",
      "Testing accuracy： 0.9493589743589743\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1371794871794872\n",
      "Testing accuracy： 0.1371794871794872\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9448717948717948\n",
      "Testing accuracy： 0.9448717948717948\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.16474358974358974\n",
      "Testing accuracy： 0.16474358974358974\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9512820512820512\n",
      "Testing accuracy： 0.9512820512820512\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.15705128205128205\n",
      "Testing accuracy： 0.15705128205128205\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9487179487179487\n",
      "Testing accuracy： 0.9487179487179487\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.12884615384615383\n",
      "Testing accuracy： 0.12884615384615383\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9423076923076923\n",
      "Testing accuracy： 0.9423076923076923\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.11217948717948718\n",
      "Testing accuracy： 0.11217948717948718\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14743589743589744\n",
      "Testing accuracy： 0.14743589743589744\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9525641025641025\n",
      "Testing accuracy： 0.9525641025641025\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.19807692307692307\n",
      "Testing accuracy： 0.19807692307692307\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9423076923076923\n",
      "Testing accuracy： 0.9423076923076923\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1955128205128205\n",
      "Testing accuracy： 0.1955128205128205\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9455128205128205\n",
      "Testing accuracy： 0.9455128205128205\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.22115384615384615\n",
      "Testing accuracy： 0.22115384615384615\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9455128205128205\n",
      "Testing accuracy： 0.9455128205128205\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.10961538461538461\n",
      "Testing accuracy： 0.10961538461538461\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.941025641025641\n",
      "Testing accuracy： 0.941025641025641\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1032051282051282\n",
      "Testing accuracy： 0.1032051282051282\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9416666666666667\n",
      "Testing accuracy： 0.9416666666666667\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.17307692307692307\n",
      "Testing accuracy： 0.17307692307692307\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9391025641025641\n",
      "Testing accuracy： 0.9391025641025641\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.11153846153846154\n",
      "Testing accuracy： 0.11153846153846154\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9474358974358974\n",
      "Testing accuracy： 0.9474358974358974\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.2608974358974359\n",
      "Testing accuracy： 0.2608974358974359\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9525641025641025\n",
      "Testing accuracy： 0.9525641025641025\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.23525641025641025\n",
      "Testing accuracy： 0.23525641025641025\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.95\n",
      "Testing accuracy： 0.95\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.08525641025641026\n",
      "Testing accuracy： 0.08525641025641026\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9461538461538461\n",
      "Testing accuracy： 0.9461538461538461\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.12884615384615383\n",
      "Testing accuracy： 0.12884615384615383\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.1576923076923077\n",
      "Testing accuracy： 0.1576923076923077\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9442307692307692\n",
      "Testing accuracy： 0.9442307692307692\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.07435897435897436\n",
      "Testing accuracy： 0.07435897435897436\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Testing accuracy： 0.9429487179487179\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.11858974358974358\n",
      "Testing accuracy： 0.11858974358974358\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9326923076923077\n",
      "Testing accuracy： 0.9326923076923077\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14615384615384616\n",
      "Testing accuracy： 0.14615384615384616\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9397435897435897\n",
      "Testing accuracy： 0.9397435897435897\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.14615384615384616\n",
      "Testing accuracy： 0.14615384615384616\n"
     ]
    }
   ],
   "source": [
    "for p_seed in np.arange(0,50):\n",
    "    orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss=cal(p_data_arr,\\\n",
    "                                                                                        p_label_arr_onehot,\\\n",
    "                                                                                        p_key_feture_number,\\\n",
    "                                                                                        p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
