{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"../Defined\")\n",
    "import Functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame=np.array(pd.read_csv('../Dataset/isolet1+2+3+4.data',header=None))\n",
    "test_data_frame=np.array(pd.read_csv('../Dataset/isolet5.data',header=None))\n",
    "\n",
    "train_data_arr=(train_data_frame[:,0:617]).copy()\n",
    "train_label_arr=((train_data_frame[:,617]).copy()-1)\n",
    "test_data_arr=(test_data_frame[:,0:617]).copy()\n",
    "test_label_arr=((test_data_frame[:,617]).copy()-1)\n",
    "\n",
    "data_arr=MinMaxScaler(feature_range=(0,1)).fit_transform(np.r_[train_data_arr,test_data_arr])\n",
    "\n",
    "label_arr_onehot=np.r_[train_label_arr,test_label_arr]#to_categorical(train_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR\n",
    " \n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def cal(p_data_arr,\\\n",
    "        p_label_arr_onehot,\\\n",
    "        p_key_feture_number,\\\n",
    "        p_epochs_number,\\\n",
    "        p_batch_size_value,\\\n",
    "        p_is_use_bias,\\\n",
    "        p_seed):\n",
    "    \n",
    "    C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(p_data_arr,p_label_arr_onehot,test_size=0.2,random_state=p_seed)\n",
    "    x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=p_seed)\n",
    "    x_test=C_test_x\n",
    "    y_test_onehot=C_test_y\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(p_seed)\n",
    "    np.random.seed(p_seed)\n",
    "    rn.seed(p_seed)\n",
    "    tf.compat.v1.set_random_seed(p_seed)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    class Feature_Select_Layer(Layer):\n",
    "    \n",
    "        def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "            super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "            self.output_dim = output_dim\n",
    "            self.l1_lambda=l1_lambda\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.kernel = self.add_weight(name='kernel',\\\n",
    "                                          shape=(input_shape[1],),\\\n",
    "                                          initializer=initializers.RandomUniform(minval=0.999999, maxval=0.9999999, seed=p_seed),\\\n",
    "                                          trainable=True,\\\n",
    "                                          regularizer=regularizers.l1(self.l1_lambda),\\\n",
    "                                          constraint=constraints.NonNeg())\n",
    "            \n",
    "            super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "        def call(self, x, selection=False,k=p_key_feture_number):\n",
    "            kernel=self.kernel        \n",
    "            if selection:\n",
    "                kernel_=K.transpose(kernel)\n",
    "                kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "                kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "            return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return (input_shape[0], self.output_dim)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    def Fractal_Autoencoder(p_data_feature,\\\n",
    "                            p_feture_number,\\\n",
    "                            p_encoding_dim,\\\n",
    "                            p_learning_rate,\\\n",
    "                            p_l1_lambda,\\\n",
    "                            p_loss_weight_1,\\\n",
    "                            p_loss_weight_2,\\\n",
    "                            p_seed):\n",
    "    \n",
    "        input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "        feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                                 l1_lambda=p_l1_lambda,\\\n",
    "                                                 input_shape=(p_data_feature,),\\\n",
    "                                                 name='feature_selection')\n",
    "\n",
    "        feature_selection_score=feature_selection(input_img)\n",
    "        feature_selection_choose=feature_selection(input_img,selection=True,k=p_feture_number)\n",
    "\n",
    "        encoded = Dense(p_encoding_dim,\\\n",
    "                        activation='linear',\\\n",
    "                        kernel_initializer=initializers.glorot_uniform(p_seed),\\\n",
    "                        use_bias=p_is_use_bias,\\\n",
    "                        name='autoencoder_hidden_layer')\n",
    "    \n",
    "        encoded_score=encoded(feature_selection_score)\n",
    "        encoded_choose=encoded(feature_selection_choose)\n",
    "    \n",
    "        bottleneck_score=encoded_score\n",
    "        bottleneck_choose=encoded_choose\n",
    "    \n",
    "        decoded = Dense(p_data_feature,\\\n",
    "                        activation='linear',\\\n",
    "                        kernel_initializer=initializers.glorot_uniform(p_seed),\\\n",
    "                        use_bias=p_is_use_bias,\\\n",
    "                        name='autoencoder_output')\n",
    "    \n",
    "        decoded_score =decoded(bottleneck_score)\n",
    "        decoded_choose =decoded(bottleneck_choose)\n",
    "\n",
    "        latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "        latent_encoder_choose = Model(input_img, bottleneck_choose)\n",
    "        feature_selection_output=Model(input_img,feature_selection_choose)\n",
    "        autoencoder = Model(input_img, [decoded_score,decoded_choose])\n",
    "    \n",
    "        autoencoder.compile(loss=['mean_squared_error','mean_squared_error'],\\\n",
    "                            loss_weights=[p_loss_weight_1, p_loss_weight_2],\\\n",
    "                            optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "        #print('Autoencoder Structure-------------------------------------')\n",
    "        #autoencoder.summary()\n",
    "        return autoencoder,feature_selection_output,latent_encoder_score,latent_encoder_choose\n",
    "    \n",
    "    t_start = time.time()\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    F_AE,\\\n",
    "    feature_selection_output,\\\n",
    "    latent_encoder_score_F_AE,\\\n",
    "    latent_encoder_choose_F_AE=Fractal_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                   p_feture_number=p_key_feture_number,\\\n",
    "                                                   p_encoding_dim=p_key_feture_number,\\\n",
    "                                                   p_learning_rate= 1E-3,\\\n",
    "                                                   p_l1_lambda=0.1,\\\n",
    "                                                   p_loss_weight_1=1,\\\n",
    "                                                   p_loss_weight_2=2,\\\n",
    "                                                   p_seed=p_seed)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    F_AE_history = F_AE.fit(x_train, [x_train,x_train],\\\n",
    "                            epochs=p_epochs_number,\\\n",
    "                            batch_size=p_batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            verbose=0,\\\n",
    "                            validation_data=(x_validate, [x_validate,x_validate]))\n",
    "    \n",
    "    t_used=time.time() - t_start\n",
    "    \n",
    "    write_to_csv(np.array([t_used]),\"./log/FAE_time_bias_\"+str(p_is_use_bias)+\".csv\")\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    p_data=F_AE.predict(x_test)\n",
    "    numbers=x_test.shape[0]*x_test.shape[1]\n",
    "    \n",
    "    print(\"Completed on \"+str(p_seed)+\"!\")\n",
    "    print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)[0]-x_test,2))/numbers)\n",
    "    print(\"MSE for feature selection layer\",np.sum(np.power(np.array(p_data)[1]-x_test,2))/numbers)\n",
    "    \n",
    "    FS_layer_output=feature_selection_output.predict(x_test)\n",
    "    key_features=F.top_k_keepWeights_1(F_AE.get_layer(index=1).get_weights()[0],key_feture_number)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Identify features successfully\n",
    "    if np.sum(F_AE.get_layer(index=1).get_weights()[0]>0)>0:\n",
    "        # Classification on original features\n",
    "        train_feature=C_train_x\n",
    "        train_label=C_train_y\n",
    "        test_feature=C_test_x\n",
    "        test_label=C_test_y\n",
    "        orig_train_acc,orig_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "    \n",
    "        # Classification on selected features\n",
    "        selected_position_list=np.where(key_features>0)[0]\n",
    "    \n",
    "        train_feature_=np.multiply(C_train_x, key_features)\n",
    "        train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "        train_label=C_train_y\n",
    "\n",
    "        test_feature_=np.multiply(C_test_x, key_features)\n",
    "        test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "        test_label=C_test_y\n",
    "        selec_train_acc,selec_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "    \n",
    "        # Linear reconstruction\n",
    "        train_feature_=np.multiply(C_train_x, key_features)\n",
    "        C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "\n",
    "        test_feature_=np.multiply(C_test_x, key_features)\n",
    "        C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "\n",
    "        train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "        test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "        reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "    \n",
    "        print(\"Classification on original data\",orig_test_acc)\n",
    "        print(\"Classification on selected features\",selec_test_acc)\n",
    "        print(\"Linear reconstruction loss\",reconstruction_loss)\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    else:\n",
    "        orig_train_acc=-1\n",
    "        orig_test_acc=-1\n",
    "        selec_train_acc=-1\n",
    "        selec_test_acc=-1\n",
    "        reconstruction_loss=-1\n",
    "        \n",
    "    results=np.array([orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss])\n",
    "    write_to_csv(results.reshape(1,len(results)),\"./log/FAE_results_bias_\"+str(p_is_use_bias)+\".csv\")\n",
    "    \n",
    "    return orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=128\n",
    "p_is_use_bias=False\n",
    "\n",
    "p_data_arr=data_arr\n",
    "p_label_arr_onehot=label_arr_onehot\n",
    "p_key_feture_number=key_feture_number\n",
    "p_epochs_number=epochs_number\n",
    "p_batch_size_value=batch_size_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p_seed in np.arange(0,50):\n",
    "    orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss=cal(p_data_arr,\\\n",
    "                                                                                        p_label_arr_onehot,\\\n",
    "                                                                                        p_key_feture_number,\\\n",
    "                                                                                        p_epochs_number,\\\n",
    "                                                                                        p_batch_size_value,\\\n",
    "                                                                                        p_is_use_bias,\\\n",
    "                                                                                        p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
