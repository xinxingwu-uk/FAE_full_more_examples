{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"../Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='../Dataset/coil-20-proc/'\n",
    "\n",
    "samples={}\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "    #print(dirpath)\n",
    "    #print(dirnames)\n",
    "    #print(filenames)\n",
    "    dirnames.sort()\n",
    "    filenames.sort()\n",
    "    for filename in [f for f in filenames if f.endswith(\".png\") and not f.find('checkpoint')>0]:\n",
    "        full_path = os.path.join(dirpath, filename)\n",
    "        file_identifier=filename.split('__')[0][3:]\n",
    "        if file_identifier not in samples.keys():\n",
    "            samples[file_identifier] = []\n",
    "        # Direct read\n",
    "        #image = io.imread(full_path)\n",
    "        # Resize read\n",
    "        image_=Image.open(full_path).resize((20, 20),Image.ANTIALIAS)\n",
    "        image=np.asarray(image_)\n",
    "        samples[file_identifier].append(image)\n",
    "        \n",
    "#plt.imshow(samples['1'][0].reshape(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr_list=[]\n",
    "label_arr_list=[]\n",
    "for key_i in samples.keys():\n",
    "    key_i_for_label=[int(key_i)-1]\n",
    "    data_arr_list.append(np.array(samples[key_i]))\n",
    "    label_arr_list.append(np.array(72*key_i_for_label))\n",
    "    \n",
    "data_arr=np.concatenate(data_arr_list).reshape(1440, 20*20).astype('float32') / 255.\n",
    "label_arr_onehot=np.concatenate(label_arr_list)#to_categorical(np.concatenate(label_arr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR\n",
    " \n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def cal(p_data_arr,\\\n",
    "        p_label_arr_onehot,\\\n",
    "        p_key_feture_number,\\\n",
    "        p_epochs_number,\\\n",
    "        p_batch_size_value,\\\n",
    "        p_seed):        \n",
    "        \n",
    "    C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(p_data_arr,p_label_arr_onehot,test_size=0.2,random_state=p_seed)\n",
    "    x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=p_seed)\n",
    "    x_test=C_test_x\n",
    "    y_test_onehot=C_test_y\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(p_seed)\n",
    "    np.random.seed(p_seed)\n",
    "    rn.seed(p_seed)\n",
    "    tf.compat.v1.set_random_seed(p_seed)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    class Feature_Select_Layer(Layer):\n",
    "    \n",
    "        def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "            super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "            self.output_dim = output_dim\n",
    "            self.l1_lambda=l1_lambda\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "            super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "        def call(self, x, selection=False,k=p_key_feture_number):\n",
    "            kernel=self.kernel        \n",
    "            if selection:\n",
    "                kernel_=K.transpose(kernel)\n",
    "                print(kernel_.shape)\n",
    "                kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "                kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "            return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return (input_shape[0], self.output_dim)\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    def Identity_Autoencoder(p_data_feature,\\\n",
    "                             p_encoding_dim,\\\n",
    "                             p_learning_rate,\\\n",
    "                             p_l1_lambda,\\\n",
    "                            p_seed=p_seed):\n",
    "    \n",
    "        input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "        feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "        feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "        encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "        encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "        bottleneck_score=encoded_score\n",
    "    \n",
    "        decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "        decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "        latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "        autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "        autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "        #print('Autoencoder Structure-------------------------------------')\n",
    "        #autoencoder.summary()\n",
    "        return autoencoder,latent_encoder_score\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    Ide_AE,\\\n",
    "    latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=p_key_feture_number,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda,\\\n",
    "                                                    p_seed=p_seed)    \n",
    "    \n",
    "    Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=p_epochs_number,\\\n",
    "                            batch_size=p_batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            verbose=0,\\\n",
    "                            validation_data=(x_validate,x_validate))\n",
    "    \n",
    "    t_used=time.time() - t_start\n",
    "    \n",
    "    write_to_csv(np.array([t_used]),\"./log/AgnoSS_time.csv\")\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    p_data=Ide_AE.predict(x_test)\n",
    "    numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "    key_number=p_key_feture_number\n",
    "    key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "    selected_position_list=np.where(key_features>0)[0]\n",
    "    \n",
    "    if np.sum(Ide_AE.get_layer(index=1).get_weights()[0]>0)>0:\n",
    "    \n",
    "        # Classification on original features\n",
    "        train_feature=C_train_x\n",
    "        train_label=C_train_y\n",
    "        test_feature=C_test_x\n",
    "        test_label=C_test_y\n",
    "        orig_train_acc,orig_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "        \n",
    "        # Classification on selected features\n",
    "        train_feature_=np.multiply(C_train_x, key_features)\n",
    "        train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "        train_label=C_train_y\n",
    "\n",
    "        test_feature_=np.multiply(C_test_x, key_features)\n",
    "        test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "        test_label=C_test_y\n",
    "\n",
    "        selec_train_acc,selec_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "\n",
    "        # Linear reconstruction\n",
    "        train_feature_=np.multiply(C_train_x, key_features)\n",
    "        C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "\n",
    "        test_feature_=np.multiply(C_test_x, key_features)\n",
    "        C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "\n",
    "        train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "        test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "        reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "\n",
    "        print(\"Classification on original data\",orig_test_acc)\n",
    "        print(\"Classification on selected features\",selec_test_acc)\n",
    "        print(\"Linear reconstruction loss\",reconstruction_loss)\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "    else:\n",
    "        orig_train_acc=-1\n",
    "        orig_test_acc=-1\n",
    "        selec_train_acc=-1\n",
    "        selec_test_acc=-1\n",
    "        reconstruction_loss=-1\n",
    "\n",
    "    results=np.array([orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss])\n",
    "\n",
    "    write_to_csv(results.reshape(1,len(results)),\"./log/AgnoSS_results.csv\")\n",
    "    \n",
    "    return orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=128\n",
    "\n",
    "p_data_arr=data_arr\n",
    "p_label_arr_onehot=label_arr_onehot\n",
    "p_key_feture_number=key_feture_number\n",
    "p_epochs_number=epochs_number\n",
    "p_batch_size_value=batch_size_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_seed in np.arange(0,50):\n",
    "    orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss=cal(p_data_arr,\\\n",
    "                                                                                        p_label_arr_onehot,\\\n",
    "                                                                                        p_key_feture_number,\\\n",
    "                                                                                        p_epochs_number,\\\n",
    "                                                                                        p_batch_size_value,\\\n",
    "                                                                                        p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
