{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import scipy.io\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"../Defined\")\n",
    "import Functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../Dataset/leukemia.mat\"\n",
    "Data = scipy.io.loadmat(data_path)\n",
    "\n",
    "data_arr_=Data['X']\n",
    "label_arr_=Data['Y'][:, 0]\n",
    "\n",
    "label_arr_=np.array([0 if label_arr_i<0 else label_arr_i for label_arr_i in label_arr_])\n",
    "\n",
    "Data=StandardScaler().fit_transform(data_arr_)\n",
    "data_arr=Data\n",
    "\n",
    "label_arr=label_arr_\n",
    "\n",
    "label_arr_onehot=label_arr#to_categorical(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def IsnanAndIsinf(p_data):\n",
    "    p_data=np.array(p_data)\n",
    "    for i in np.arange(p_data.shape[0]):\n",
    "        for j in np.arange(p_data.shape[1]):\n",
    "            if np.isnan(p_data[i,j]) or np.isinf(p_data[i,j]):\n",
    "                p_data[i,j]=0\n",
    "    return p_data\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def InfFS(p_data_arr,p_alpha,use_specify_number=False,specify_number=50):\n",
    "    df = pd.DataFrame(p_data_arr)\n",
    "    corr_ij_spearman__=df.corr(method ='spearman')\n",
    "    corr_ij_spearman_=IsnanAndIsinf(corr_ij_spearman__)\n",
    "    corr_ij_spearman=1-np.abs(corr_ij_spearman_)\n",
    "    \n",
    "    STD=np.std(p_data_arr,axis=0)\n",
    "    \n",
    "    STDMatrix_=np.zeros((STD.shape[0],STD.shape[0]))\n",
    "    for i in np.arange(STD.shape[0]):\n",
    "        for j in np.arange(STD.shape[0]):\n",
    "            STDMatrix_[i,j]=max(STD[i],STD[j])\n",
    "            \n",
    "    STDMatrix_min=STDMatrix_-np.min(STDMatrix_)\n",
    "    STDMatrix_max=np.max(STDMatrix_min)\n",
    "    STDMatrix__=STDMatrix_min/STDMatrix_max\n",
    "    \n",
    "    STDMatrix=IsnanAndIsinf(STDMatrix__)\n",
    "    \n",
    "    N=p_data_arr.shape[1]\n",
    "    \n",
    "    eps = (5e-06) * N;\n",
    "    factor = 1 - eps\n",
    "    \n",
    "    A =  ( p_alpha*STDMatrix + (1-p_alpha)*corr_ij_spearman )\n",
    "\n",
    "    rho = np.max(np.sum(A,axis=1))\n",
    "\n",
    "    A = A / (rho+eps)\n",
    "    \n",
    "    I = np.eye(A.shape[0])\n",
    "    \n",
    "    r = factor/rho\n",
    "    \n",
    "    y = I - ( r * A )\n",
    "    S=np.linalg.inv(y)\n",
    "    \n",
    "    WEIGHT = np.sum( S , axis=1 )\n",
    "    RANKED=np.argsort(-WEIGHT)\n",
    "    \n",
    "    RANKED = RANKED\n",
    "    WEIGHT = WEIGHT\n",
    "    \n",
    "    e = np.ones(N)\n",
    "    t = np.dot(S, e)\n",
    "\n",
    "    nbins = 0.5*N\n",
    "    \n",
    "    cnts, bins = np.histogram(t, bins=int(nbins))\n",
    "    \n",
    "    thr =np.mean(cnts)\n",
    "    \n",
    "    size_sub = np.sum(cnts>thr)\n",
    "    \n",
    "    if use_specify_number:\n",
    "        size_sub=specify_number\n",
    "    \n",
    "    SUBSET = RANKED[0:size_sub]\n",
    "    \n",
    "    return SUBSET\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------       \n",
    "def cal(p_data_arr,\\\n",
    "        p_label_arr_onehot,\\\n",
    "        p_key_feture_number,\\\n",
    "        p_seed):\n",
    "    C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(p_data_arr,p_label_arr_onehot,test_size=0.2,random_state=p_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(p_seed)\n",
    "    np.random.seed(p_seed)\n",
    "    rn.seed(p_seed)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    train_feature=C_train_x\n",
    "    test_feature=C_test_x\n",
    "\n",
    "    t_start = time.time()\n",
    "    \n",
    "    train_idx=InfFS(train_feature,p_alpha,use_specify_number=True,specify_number=p_key_feture_number)\n",
    "\n",
    "    t_used=time.time() - t_start\n",
    "    \n",
    "    write_to_csv(np.array([t_used]),\"./log/InfFS_time.csv\")\n",
    "    \n",
    "    C_train_selected_x = train_feature[:, train_idx]\n",
    "    \n",
    "    test_idx=InfFS(test_feature,p_alpha,use_specify_number=True,specify_number=p_key_feture_number)\n",
    "    C_test_selected_x = test_feature[:, test_idx]\n",
    "\n",
    "    # Classification on original features\n",
    "    train_feature=C_train_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_x\n",
    "    test_label=C_test_y\n",
    "    \n",
    "    orig_train_acc,orig_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "    \n",
    "    # Classification on selected features\n",
    "    train_feature=C_train_selected_x\n",
    "    train_label=C_train_y\n",
    "    test_feature=C_test_selected_x\n",
    "    test_label=C_test_y\n",
    "\n",
    "    selec_train_acc,selec_test_acc=F.ETree(train_feature,train_label,test_feature,test_label,0)\n",
    "\n",
    "    # Linear reconstruction\n",
    "    train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "    test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "    reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "    results=np.array([orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss])\n",
    "    \n",
    "    write_to_csv(results.reshape(1,len(results)),\"./log/InfFS_results.csv\")\n",
    "    \n",
    "    return orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_arr=data_arr\n",
    "p_alpha=0.5\n",
    "p_label_arr_onehot=label_arr_onehot\n",
    "p_key_feture_number=key_feture_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p_seed in np.arange(0,50):\n",
    "    orig_train_acc,orig_test_acc,selec_train_acc,selec_test_acc,reconstruction_loss=cal(p_data_arr,\\\n",
    "                                                                                        p_label_arr_onehot,\\\n",
    "                                                                                        p_key_feture_number,\\\n",
    "                                                                                        p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
